{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Recognition Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to intall some of the libraries which are not already in Jupiter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/44/e1/dc0757b20b56c980b5553c1b5c4c32d378c7055ab7bfa92006801ad359ab/Keras-2.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (1.16.5)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from h5py->keras) (1.12.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here import all the useful libraries and loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 27s 2us/step\n",
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   #load the mnist dataset\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling/Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before coming to the training part we will initialize our dataset and make it able to run through the CNN model\n",
    "\n",
    "We have 60000 training samples and 10000 testing samples.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)     #adding 1 more dimention to the data to make it for CNN model\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255      #normalization of data\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create our CNN model, we use to perform pooling operation which outputs a downsized version of the input.\n",
    "\n",
    "We also use ReLu function -which is used to provide output directly if input is positive.\n",
    "\n",
    "And we will drop the some neurons which are just making the condition of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128      #number of training examples in one batch\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "model = Sequential()    #appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))  #activating relu function\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))  #here max pooling used\n",
    "model.add(Dropout(0.25))  #droptout neurons\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is most important part ,it will take some time to train the data , uses training data to train the model.\n",
    "\n",
    "model.fit() is used for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 70s 149ms/step - loss: 2.2743 - accuracy: 0.1703 - val_loss: 2.2255 - val_accuracy: 0.4169\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 72s 154ms/step - loss: 2.1935 - accuracy: 0.3339 - val_loss: 2.1237 - val_accuracy: 0.6187\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 73s 155ms/step - loss: 2.0835 - accuracy: 0.4618 - val_loss: 1.9799 - val_accuracy: 0.6873\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 72s 153ms/step - loss: 1.9321 - accuracy: 0.5457 - val_loss: 1.7835 - val_accuracy: 0.7277\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 70s 150ms/step - loss: 1.7326 - accuracy: 0.6000 - val_loss: 1.5448 - val_accuracy: 0.7616\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 76s 161ms/step - loss: 1.5167 - accuracy: 0.6350 - val_loss: 1.2973 - val_accuracy: 0.7841\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 76s 161ms/step - loss: 1.3102 - accuracy: 0.6692 - val_loss: 1.0800 - val_accuracy: 0.8059\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 78s 167ms/step - loss: 1.1417 - accuracy: 0.6942 - val_loss: 0.9111 - val_accuracy: 0.8212\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 70s 148ms/step - loss: 1.0130 - accuracy: 0.7206 - val_loss: 0.7872 - val_accuracy: 0.8334\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 69s 147ms/step - loss: 0.9203 - accuracy: 0.7347 - val_loss: 0.6980 - val_accuracy: 0.8421\n",
      "The model has successfully trained\n",
      "Saving the model as mnist.h5\n"
     ]
    }
   ],
   "source": [
    "#model.fit() is used to train the data\n",
    "hist = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
    "print(\"The model has successfully trained\")\n",
    "model.save('mnist.h5')\n",
    "print(\"Saving the model as mnist.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally come to Testing Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will show the acuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6980320811271667\n",
      "Test accuracy: 0.8421000242233276\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)  #evalute the model\n",
    "print('Test loss:', score[0])  #gives the loss in testing\n",
    "print('Test accuracy:', score[1])   #gives the accuracy of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create our interface where you can write any digit and get the predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "import win32gui\n",
    "from PIL import ImageGrab, Image\n",
    "import numpy as np\n",
    "model = load_model('mnist.h5')\n",
    "def predict_digit(img):\n",
    "    \n",
    "    img = img.resize((28,28))  #resize image to 28x28 pixels\n",
    "    img = img.convert('L')    #convert rgb to grayscale\n",
    "    img = np.array(img)\n",
    "    img = img.reshape(1,28,28,1)    #reshaping to support our model input and normalizing\n",
    "    img = img/255.0 \n",
    "    res = model.predict([img])[0]    #predicting the class\n",
    "    \n",
    "    return np.argmax(res), max(res)\n",
    "\n",
    "class App(tk.Tk):\n",
    "    def __init__(self):\n",
    "        tk.Tk.__init__(self)\n",
    "        self.x = self.y = 0\n",
    "        self.canvas = tk.Canvas(self, width=300, height=300, bg = \"white\", cursor=\"cross\")   # Creating elements\n",
    "        self.label = tk.Label(self, text=\"Thinking..\", font=(\"Helvetica\", 48))\n",
    "        self.classify_btn = tk.Button(self, text = \"Recognise\", command =  self.classify_handwriting) \n",
    "        self.button_clear = tk.Button(self, text = \"Clear\", command = self.clear_all)\n",
    "        # Grid structure\n",
    "        self.canvas.grid(row=0, column=0, pady=2, sticky=W, )\n",
    "        self.label.grid(row=0, column=1,pady=2, padx=2)\n",
    "        self.classify_btn.grid(row=1, column=1, pady=2, padx=2)\n",
    "        self.button_clear.grid(row=1, column=0, pady=2)\n",
    "        self.canvas.bind(\"<B1-Motion>\", self.draw_lines)  #self.canvas.bind(\"<Motion>\", self.start_pos)\n",
    "        \n",
    "    def clear_all(self):\n",
    "        self.canvas.delete(\"all\")\n",
    "        \n",
    "    def classify_handwriting(self):\n",
    "        HWND = self.canvas.winfo_id() # get the handle of the canvas\n",
    "        rect = win32gui.GetWindowRect(HWND) # get the coordinate of the canvas\n",
    "        im = ImageGrab.grab(rect)\n",
    "        digit, acc = predict_digit(im)\n",
    "        self.label.configure(text= str(digit)+', '+ str(int(acc*100))+'%')\n",
    "        \n",
    "    def draw_lines(self, event):\n",
    "        self.x = event.x\n",
    "        self.y = event.y\n",
    "        r=8\n",
    "        self.canvas.create_oval(self.x-r, self.y-r, self.x + r, self.y + r, fill='black')\n",
    "app = App()\n",
    "mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
